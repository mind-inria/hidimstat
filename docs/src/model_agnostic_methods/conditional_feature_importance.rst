.. _conditional_feature_importance:


==============================
Conditional Feature Importance
==============================

Conditional Feature Importance (CFI) is a model-agnostic approach for quantifying the 
relevance of individual or groups of features in predictive models. It is a 
perturbation-based method that compares the predictive performance of a model on 
unmodified test data—following the same distribution as the training data—
to its performance when the studied feature is conditionally perturbed. Thus, this approach 
does not require retraining the model.

.. figure:: ../generated/gallery/examples/images/sphx_glr_plot_cfi_001.png
    :target: ../generated/gallery/examples/plot_cfi.html
    :align: center


Theoretical index
------------------

Conditional Feature Importance (CFI) is a model-agnostic method for estimating feature 
importance through conditional perturbations. Specifically, it constructs a perturbed 
version of the feature :math:`X_j^P`, sampled independently from the conditional distribution 
:math:`P(X_j | X_{-j})`, such that its association with the output is removed: 
:math:`X_j^P \perp Y \mid X^{-j}`. The predictive model is then evaluated on the 
modified feature vector :math:`\tilde X = [X_1, ..., X_j^P, ..., X_p]`, and the 
importance of the feature is quantified by the resulting drop in model performance.

.. math::
    \psi_j^{CFI} = \mathbb{E} [\mathcal{L}(y, \mu(\tilde X))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].


The target quantity estimated by CFI is the Total Sobol Index (TSI) :ref:`total_sobol_index`. 
Indeed, 

.. math::
    \frac{1}{2} \psi_j^{CFI} 
    = \psi_j^{TSI} 
    = \mathbb{E} [\mathcal{L}(y, \mu_{-j}(X^-j))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].

Where in regression, :math:`\mu_{-j}(X_{-j}) = \mathbb{E}[Y| X_{-j}]` is the 
theoretical model without the :math:`j^{th}` feature.

Estimation procedure
--------------------

The estimation of CFI relies on the ability to sample the perturbed feature matrix 
:math:`\tilde X`, and specifically to sample :math:`X_j^p` independently from the conditional 
distribution, :math:`X_j^p \overset{\text{i.i.d.}}{\sim} P(X_j | X_{-j})`, while breaking the
association with the output :math:`Y`. Any conditional sampler can be used. A valid 
and efficient approach is conditional permutation (:footcite:t:`Chamma_NeurIPS2023`). 
This procedure decomposes the :math:`j^{th}` feature into a part that 
is predictable from the other features and a residual term that is 
independent of the other features:

.. math::
    X_j = \nu_j(X_{-j}) + \epsilon_j, \quad \text{with} \quad \epsilon_j \perp\!\!\!\perp X_{-j} \text{ and } \mathbb{E}[\epsilon_j] = 0.

Here :math:`\nu_j(X_{-j}) = \mathbb{E}[X_j | X_{-j}]` is the conditional expectation of
:math:`X_j` given the other features. In practice, :math:`\nu_j` is unknown and has to be
estimated from the data using a predictive model. 

Then the perturbed feature :math:`X_j^p` is generated by keeping the predictable part
:math:`\nu_j(X_{-j})` unchanged, and by replacing the residual :math:`\epsilon_j` by a
randomly permuted version :math:`\epsilon_j^p`:

.. math::
    X_j^p = \nu_j(X_{-j}) + \epsilon_j^p, \quad \text{with} \quad \epsilon_j^p \sim \text{Perm}(\epsilon_j).


.. note:: **Estimation of** :math:`\nu_j`

    To generate the perturbed feature :math:`X_j^p`, a model for :math:`\nu_j` is required.
    Estimating :math:`\nu_j` amounts to modeling the relationship between features and is
    arguably an easier task than estimating the relationship between features and the 
    target. This 'model-X' assumption was for instance argued in :footcite:t:`Chamma_NeurIPS2023`, 
    :footcite:t:`candes2018panning`. 
    For example, in genetics, features such as single nucleotide polymorphisms (SNPs) 
    are the basis of complex biological processes that result in an outcome (phenotype), 
    such as a disease. Predicting the phenotype from SNPs is challenging, whereas 
    modeling the relationships between SNPs is often easier due to known correlation 
    structures in the genome (linkage disequilibrium). As a result, simple predictive 
    models such as regularized linear models or decision trees can be used to estimate 
    :math:`\nu_j`.


Inference
---------
Under standard assumptions such as additive model: :math:`Y = \mu(X) + \epsilon`, 
Conditional Feature Importance (CFI) allows for conditional independence testing, which 
determines if a feature provides any unique information to the model's predictions that 
isn't already captured by the other features. Essentially, we are testing whether the output is independent from the studied feature given the rest of the input:

.. math::
    \mathcal{H}_0: Y \perp\!\!\!\perp X_j | X_{-j}.


The core of this inference is to test the statistical significance of the loss 
differences estimated by CFI. Consequently, a one-sample test on the loss differences
(or a paired test on the losses) needs to be performed. 

Two technical challenges arise in this context:

* When cross-validation (for instance, k-fold) is used to estimate CFI, the loss
  differences obtained from different folds are not independent. Consequently,
  performing a simple t-test on the loss differences is not valid. This issue can be
  addressed by a corrected t-test accounting for this dependence, such as the one
  proposed in :footcite:t:`nadeau1999inference`.
* Vanishing variance: Under the null hypothesis, even if the loss difference
  converges to zero, the variance of the loss differences also vanishes due to the quadratic functional (:footcite:t:verdinelli2024feature``) . This makes the
  standard one-sample t-test invalid. This second issue can be handled by correcting
  the variance estimate or using other nonparametric test.


Regression example
------------------
The following example illustrates the use of CFI on a regression task with::

    >>> from sklearn.datasets import make_regression
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI


    >>> X, y = make_regression(n_features=2)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = LinearRegression().fit(X_train, y_train)
    
    >>> cfi = CFI(estimator=model, imputation_model_continuous=LinearRegression())
    >>> cfi = cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)


Classification example
----------------------
To measure feature importance in a classification task, a classification loss should be
used, in addition, the prediction method of the estimator should output the corresponding 
type of prediction (probabilities or classes). The following example illustrates the use
of CFI on a classification task::

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.metrics import log_loss
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI

    >>> X, y = make_classification(n_features=4)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = RandomForestClassifier().fit(X_train, y_train)
    >>> cfi = CFI(
    ...     estimator=model,
    ...     imputation_model_continuous=LinearRegression(),
    ...     loss=log_loss,
    ...     method="predict_proba",
    ... )
    >>> cfi = cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)

References
----------
.. footbibliography::
