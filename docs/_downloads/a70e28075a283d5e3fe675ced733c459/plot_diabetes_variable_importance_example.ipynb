{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variable Importance on diabetes dataset\n\nVariable Importance estimates the influence of a given input variable to the\nprediction made by a model. To assess variable importance in a prediction\nproblem, :footcite:t:`breimanRandomForests2001` introduced the permutation\napproach where the values are shuffled for one variable/column at a time. This\npermutation breaks the relationship between the variable of interest and the\noutcome. Following, the loss score is checked before and after this\nsubstitution for any significant drop in the performance which reflects the\nsignificance of this variable to predict the outcome. This ease-to-use solution\nis demonstrated, in the work by\n:footcite:t:`stroblConditionalVariableImportance2008`, to be affected by the\ndegree of correlation between the variables, thus biased towards truly\nnon-significant variables highly correlated with the significant ones and\ncreating fake significant variables. They introduced a solution for the Random\nForest estimator based on conditional sampling by performing sub-groups\npermutation when bisecting the space using the conditioning variables of the\nbuiding process. However, this solution is exclusive to the Random Forest and\nis costly with high-dimensional settings.\n:footcite:t:`Chamma_NeurIPS2023` introduced a new model-agnostic solution to\nbypass the limitations of the permutation approach under the use of the\nconditional schemes. The variable of interest does contain two types of\ninformation: 1) the relationship with the remaining variables and 2) the\nrelationship with the outcome. The standard permutation, while breaking the\nrelationship with the outcome, is also destroying the dependency with the\nremaining variables. Therefore, instead of directly permuting the variable of\ninterest, the variable of interest is predicted by the remaining\nvariables and the residuals of this prediction are permuted before\nreconstructing the new version of the variable. This solution preserves the\ndependency with the remaining variables.\n\nIn this example, we compare both the standard permutation and its conditional\nvariant approaches for variable importance on the diabetes dataset for the\nsingle-level case. The aim is to see if integrating the new\nstatistically-controlled solution has an impact on the results.\n\n## References\n.. footbibliography::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports needed for this script\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom sklearn.base import clone\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeCV\nfrom sklearn.metrics import r2_score, root_mean_squared_error\nfrom sklearn.model_selection import KFold\n\nfrom hidimstat import CPI, LOCO, PermutationImportance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the diabetes dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n# Encode sex as binary\nX[:, 1] = (X[:, 1] > 0.0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a baseline model on the diabetes dataset\nWe use a Ridge regression model with a 10-fold cross-validation to fit the\ndiabetes dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_folds = 5\nregressor = RidgeCV(alphas=np.logspace(-3, 3, 10))\nregressor_list = [clone(regressor) for _ in range(n_folds)]\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    regressor_list[i].fit(X[train_index], y[train_index])\n    score = r2_score(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n    mse = root_mean_squared_error(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n\n    print(f\"Fold {i}: {score}\")\n    print(f\"Fold {i}: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a baselien model on the diabetes dataset\nWe use a Ridge regression model with a 10-fold cross-validation to fit the\ndiabetes dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_folds = 10\nregressor = RidgeCV(alphas=np.logspace(-3, 3, 10))\nregressor_list = [clone(regressor) for _ in range(n_folds)]\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    regressor_list[i].fit(X[train_index], y[train_index])\n    score = r2_score(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n    mse = root_mean_squared_error(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n\n    print(f\"Fold {i}: {score}\")\n    print(f\"Fold {i}: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the CPI method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cpi_importance_list = []\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    cpi = CPI(\n        estimator=regressor_list[i],\n        imputation_model_continuous=RidgeCV(alphas=np.logspace(-3, 3, 10)),\n        imputation_model_categorical=LogisticRegressionCV(Cs=np.logspace(-2, 2, 10)),\n        # covariate_estimator=HistGradientBoostingRegressor(random_state=0,),\n        n_permutations=50,\n        random_state=0,\n        n_jobs=4,\n    )\n    cpi.fit(X_train, y_train)\n    importance = cpi.importance(X_test, y_test)\n    cpi_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the LOCO method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loco_importance_list = []\n\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    loco = LOCO(\n        estimator=regressor_list[i],\n        n_jobs=4,\n    )\n    loco.fit(X_train, y_train)\n    importance = loco.importance(X_test, y_test)\n    loco_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the permutation method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pi_importance_list = []\n\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    pi = PermutationImportance(\n        estimator=regressor_list[i],\n        n_permutations=50,\n        random_state=0,\n        n_jobs=4,\n    )\n    pi.fit(X_train, y_train)\n    importance = pi.importance(X_test, y_test)\n    pi_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a function to compute the p-value from importance values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_pval(vim):\n    mean_vim = np.mean(vim, axis=0)\n    std_vim = np.std(vim, axis=0)\n    pval = norm.sf(mean_vim / std_vim)\n    return np.clip(pval, 1e-10, 1 - 1e-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cpi_vim_arr = np.array([x[\"importance\"] for x in cpi_importance_list]) / 2\ncpi_pval = compute_pval(cpi_vim_arr)\n\nvim = [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(cpi_vim_arr.shape[1]),\n            \"importance\": x[\"importance\"],\n            \"fold\": i,\n            \"pval\": cpi_pval,\n            \"method\": \"CPI\",\n        }\n    )\n    for x in cpi_importance_list\n]\n\nloco_vim_arr = np.array([x[\"importance\"] for x in loco_importance_list])\nloco_pval = compute_pval(loco_vim_arr)\n\nvim += [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(loco_vim_arr.shape[1]),\n            \"importance\": x[\"importance\"],\n            \"fold\": i,\n            \"pval\": loco_pval,\n            \"method\": \"LOCO\",\n        }\n    )\n    for x in loco_importance_list\n]\n\npi_vim_arr = np.array([x[\"importance\"] for x in pi_importance_list])\npi_pval = compute_pval(pi_vim_arr)\n\nvim += [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(pi_vim_arr.shape[1]),\n            \"importance\": x[\"importance\"],\n            \"fold\": i,\n            \"pval\": pi_pval,\n            \"method\": \"PI\",\n        }\n    )\n    for x in pi_importance_list\n]\n\nfig, ax = plt.subplots()\ndf_plot = pd.concat(vim)\ndf_plot[\"pval\"] = -np.log10(df_plot[\"pval\"])\nmethods = df_plot[\"method\"].unique()\ncolors = plt.cm.get_cmap(\"tab10\", 10)\n\nfor i, method in enumerate(methods):\n    subset = df_plot[df_plot[\"method\"] == method]\n    ax.bar(\n        subset[\"var\"] + i * 0.2,\n        subset[\"pval\"],\n        width=0.2,\n        label=method,\n        color=colors(i),\n    )\n\nax.legend(title=\"Method\")\nax.set_ylabel(r\"$-\\log_{10}(\\text{p-value})$\")\nax.axhline(-np.log10(0.05), color=\"tab:red\", ls=\"--\")\nax.set_xlabel(\"Variable\")\nax.set_xticklabels(diabetes.feature_names)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}