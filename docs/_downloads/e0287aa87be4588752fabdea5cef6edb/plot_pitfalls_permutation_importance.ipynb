{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pitfalls of Permutation Feature Importance (PFI) on the California Housing Dataset\n\nIn this example, we illustrate the pitfalls of using permutation feature importance\n(PFI) on the California housing dataset.\nPFI measures the importance of a variable. However, it does not measure conditional\nimportance and does not provide statistical control over the risk of making false\ndiscoveries, i.e., the risk of declaring a variable as important when it is not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom scipy.stats import ttest_1samp\nfrom sklearn.base import clone\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hidimstat import CPI, PFI\nfrom hidimstat.conditional_sampling import ConditionalSampler\n\nrng = np.random.RandomState(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the California housing dataset and add a spurious feature\nThe California housing dataset is a regression dataset with 8 features. We add a\nspurious feature that is a linear combination of 3 features plus some noise.\nThe spurious feature does not provide any additional information about the target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = fetch_california_housing()\nX_, y_ = dataset.data, dataset.target\n# only use 2/3 of samples to speed up the example\nX, _, y, _ = train_test_split(X_, y_, test_size=0.6667, random_state=0, shuffle=True)\n\nredundant_coef = rng.choice(np.arange(X.shape[1]), size=(3,), replace=False)\nX_spurious = X[:, redundant_coef].sum(axis=1)\nX_spurious += rng.normal(0, scale=np.std(X_spurious) * 0.5, size=X.shape[0])\nX = np.hstack([X, X_spurious[:, np.newaxis]])\nfeature_names = dataset.feature_names + [\"Spurious\"]\nprint(f\"The dataset contains {X.shape[0]} samples and {X.shape[1]} features.\")\n\n# Compute the correlation matrix\ncorrelation_matrix = np.corrcoef(X, rowvar=False)\n\n# Plot the lower triangle of the correlation matrix\nfig, ax = plt.subplots()\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\nsns.heatmap(\n    correlation_matrix,\n    mask=mask,\n    cmap=\"coolwarm\",\n    annot=True,\n    fmt=\".2f\",\n    square=True,\n    cbar_kws={\"shrink\": 0.8},\n    ax=ax,\n)\nax.set_title(\"Correlation Matrix\")\nax.set_yticks(\n    np.arange(len(feature_names)) + 0.5, labels=feature_names, fontsize=10, rotation=45\n)\nax.set_xticks(\n    np.arange(len(feature_names)) + 0.5, labels=feature_names, fontsize=10, rotation=45\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a predictive model\nWe fit a neural network model to the California housing dataset. PFI is a\nmodel-agnostic method, we therefore illustrate its behavior when using a neural\nnetwork model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitted_estimators = []\nscores = []\nmodel = TransformedTargetRegressor(\n    regressor=make_pipeline(\n        StandardScaler(),\n        MLPRegressor(\n            random_state=0,\n            hidden_layer_sizes=(32, 16, 8),\n            early_stopping=True,\n            learning_rate_init=0.01,\n            n_iter_no_change=5,\n        ),\n    ),\n    transformer=StandardScaler(),\n)\n\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_c = clone(model)\n\n    model_c = model_c.fit(X_train, y_train)\n    fitted_estimators.append(model_c)\n    y_pred = model_c.predict(X_test)\n    scores.append(r2_score(y_test, y_pred))\n\nprint(f\"Cross-validation R2 score: {np.mean(scores):.3f} \u00b1 {np.std(scores):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the PFI method\nWe use the `PermutationFeatureImportance` class to compute the PFI in a cross-fitting\nway. We then derive a p-value from importance scores using a one-sample t-test.\nAs shown in the figure below, the PFI method does not provide valid p-values for\ntesting conditional importance, as it identifies the spurious feature as important.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "permutation_importances = []\nconditional_permutation_importances = []\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model_c = fitted_estimators[i]\n\n    # Compute permutation feature importance\n    pfi = PFI(\n        model_c,\n        n_permutations=50,\n        random_state=0,\n    )\n    pfi.fit(X_test, y_test)\n\n    permutation_importances.append(pfi.importance(X_test, y_test)[\"importance\"])\npermutation_importances = np.stack(permutation_importances)\npval_pfi = ttest_1samp(\n    permutation_importances, 0.0, axis=0, alternative=\"greater\"\n).pvalue\n\n\n# Define a p-value threshold\npval_threshold = 0.05\n# Create a horizontal boxplot of permutation importances\nfig, ax = plt.subplots()\nsns.barplot(\n    permutation_importances,\n    orient=\"h\",\n    color=\"tab:blue\",\n    capsize=0.2,\n)\nax.set_xlabel(\"Permutation Importance\")\n# Add asterisks for features with p-values below the threshold\nfor i, pval in enumerate(pval_pfi):\n    if pval < pval_threshold:\n        ax.scatter(\n            np.max(permutation_importances[:, i]) + 0.01,\n            i,\n            color=\"red\",\n            marker=\"*\",\n            label=\"pvalue < 0.05\" if i == 0 else \"\",\n        )\nax.axvline(x=0, color=\"black\", linestyle=\"--\")\n# Add legend for asterisks\nax.legend(loc=\"upper right\")\nsns.despine(ax=ax)\nax.set_yticks(range(len(feature_names)), labels=feature_names)\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the most important variables identified by PFI are plausible, such as the\ngeographic coordinates or the median income of the block group, it is not robust to\nthe presence of spurious features and misleadingly identifies the spurious feature as\nimportant.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A valid alternative: Condional permutation importance\nThe `ConditionalPermutationFeatureImportance` class computes permutations of the feature of\ninterest while conditioning on the other features. In other words, it shuffles the\nintrinsic information of the feature of interest while leaving the information that is\nexplained by the other features unchanged. This method is valid for testing conditional\nimportance. As shown below, it does not identify the spurious feature as important.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditional_importances = []\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model_c = fitted_estimators[i]\n\n    # Compute conditional permutation feature importance\n    cpi = CPI(\n        model_c,\n        imputation_model_continuous=RidgeCV(alphas=np.logspace(-3, 3, 5)),\n        random_state=0,\n        n_jobs=5,\n    )\n    cpi.fit(X_test, y_test)\n\n    conditional_importances.append(cpi.importance(X_test, y_test)[\"importance\"])\n\n\ncpi_pval = ttest_1samp(\n    conditional_importances, 0.0, axis=0, alternative=\"greater\"\n).pvalue\n\n\ndf_pval = pd.DataFrame(\n    {\n        \"pval\": np.concatenate([pval_pfi, cpi_pval]),\n        \"method\": [\"PFI\"] * len(pval_pfi) + [\"CPI\"] * len(cpi_pval),\n        \"variable\": feature_names * 2,\n        \"log_pval\": -np.concatenate([np.log10(pval_pfi), np.log10(cpi_pval)]),\n    }\n)\n\n\nfig, ax = plt.subplots()\nsns.barplot(\n    data=df_pval,\n    x=\"log_pval\",\n    y=\"variable\",\n    hue=\"method\",\n    palette=\"muted\",\n    ax=ax,\n)\nax.axvline(x=-np.log10(pval_threshold), color=\"red\", linestyle=\"--\")\nax.set_xlabel(\"-$\\\\log_{10}(pval)$\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contrary to PFI, CPI does not identify the spurious feature as important.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extrapolation bias in PFI\nOne of the main pitfalls of PFI is that it leads to extrapolation bias, i.e., it\nforces the model to predict from regions of the feature space that are not present in\nthe training data. This can be seen on the california housing dataset, by comparing\nthe original latitude and longitude values with the permuted values. Indeed,\npermuting the longitude values leads to generating combinations of latitude and\nlongitude that fall outside of the borders of California and therefore are by\ndefinition not in the training data. This is not the case for the conditional\npermutation that generates perturbed but reasonable values of longitude.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test = train_test_split(\n    X,\n    test_size=0.3,\n    random_state=0,\n)\n\nconditional_sampler = ConditionalSampler(\n    model_regression=RidgeCV(alphas=np.logspace(-3, 3, 5)),\n    random_state=0,\n)\n\n\nconditional_sampler.fit(X_train[:, :7], X_train[:, 7])\nX_test_sample = conditional_sampler.sample(\n    X_test[:, :7], X_test[:, 7], n_samples=1\n).ravel()\n# sphinx_gallery_thumbnail_number = 4\nfig, ax = plt.subplots()\n\nsns.histplot(\n    x=X_test[:, 6],\n    y=X_test[:, 7],\n    color=\"tab:blue\",\n    ax=ax,\n    alpha=0.9,\n)\nsns.scatterplot(\n    x=X_test[:, 6],\n    y=X_test_sample,\n    ax=ax,\n    alpha=0.2,\n    c=\"tab:green\",\n)\nsns.scatterplot(\n    x=X_test[:, 6],\n    y=rng.permutation(X_test[:, 7]),\n    ax=ax,\n    alpha=0.2,\n    c=\"tab:orange\",\n)\n\nlegend_elements = [\n    Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=\"tab:blue\",\n        markersize=10,\n        label=\"Original\",\n    ),\n    Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=\"tab:orange\",\n        markersize=10,\n        label=\"Permutation\",\n    ),\n    Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=\"tab:green\",\n        markersize=10,\n        label=\"Conditional Permutation\",\n    ),\n]\nax.legend(handles=legend_elements, loc=\"upper right\")\nax.set_ylim(X[:, 7].min() - 0.1, X[:, 7].max() + 0.1)\nsns.despine(ax=ax)\nax.set_xlabel(\"Latitude\")\nax.set_ylabel(\"Longitude\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PFI is likely to generate samples that are unrealistic and outside of the training\ndata, leading to extrapolation bias. In contrast, CPI generates samples that respect\nthe conditional distribution of the feature of interest.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}