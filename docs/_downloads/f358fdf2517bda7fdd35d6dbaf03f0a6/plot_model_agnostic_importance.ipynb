{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variable Selection Under Model Misspecification\n\nIn this example, we illustrate the limitations of variable selection methods based on\nlinear models using the circles dataset. We first use the distilled conditional\nrandomization test (d0CRT), which is based on linear models :footcite:t:`liu2022fast` and then\ndemonstrate how model-agnostic methods, such as Leave-One-Covariate-Out (LOCO), can\nidentify important variables even when classes are not linearly separable.\n\nTo evaluate the importance of a variable, LOCO re-fits a sub-model using a subset of the\ndata where the variable of interest is removed. The importance of the variable is\nquantified as the difference in loss between the full model and the sub-model. As shown\nin :footcite:t:`williamson_2021_nonparametric` , this loss difference can be interpreted as an\nunnormalized generalized ANOVA (difference of R\u00b2).  Denoting $\\mu$ the predictive\nmodel used, $\\mu_{-j}$ the sub-model where the j-th variable is removed, and\n$X^{-j}$ the data with the j-th variable removed, the loss difference can be\nexpressed as:\n\n\\begin{align}\\psi_{j} = \\mathbb{V}(y) \\left[ \\left[ 1 - \\frac{\\mathbb{E}[(y - \\mu(X))^2]}{\\mathbb{V}(y)} \\right] - \\left[ 1 - \\frac{\\mathbb{E}[(y - \\mu_{-j}(X^{-j}))^2]}{\\mathbb{V}(y)} \\right] \\right]\\end{align}\n\nwhere $\\psi_{j}$ is the LOCO importance of the j-th variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import ttest_1samp\nfrom sklearn.base import clone\nfrom sklearn.datasets import make_circles\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import hinge_loss, log_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\n\nfrom hidimstat import LOCO, dcrt_pvalue, dcrt_zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate data where classes are not linearly separable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(0)\nX, y = make_circles(n_samples=500, noise=0.1, factor=0.6, random_state=rng)\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(\n    x=X[:, 0],\n    y=X[:, 1],\n    hue=y,\n    ax=ax,\n    palette=\"muted\",\n)\nax.legend(title=\"Class\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute p-values using d0CRT\nWe first compute the p-values using d0CRT which performs a conditional independence\ntest ($H_0: X_j \\perp\\!\\!\\!\\perp y | X_{-j}$) for each variable. However,\nthis test is based on a linear model (LogisticRegression) and fails to reject the null\nin the presence of non-linear relationships.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "selection_features, X_residual, sigma2, y_res = dcrt_zero(\n    X, y, problem_type=\"classification\", screening=False\n)\n_, pval_dcrt, _ = dcrt_pvalue(\n    selection_features=selection_features,\n    X_res=X_residual,\n    y_res=y_res,\n    sigma2=sigma2,\n    fdr=0.05,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute p-values using LOCO\nWe then compute the p-values using LOCO\nwith a linear, and then a non-linear model. When using a\nmisspecified model, such as a linear model for this dataset, LOCO fails to reject the null\nsimilarly to d0CRT. However, when using a non-linear model (SVC), LOCO is able to\nidentify the important variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=0)\nnon_linear_model = SVC(kernel=\"rbf\", random_state=0)\nlinear_model = LogisticRegressionCV(Cs=np.logspace(-3, 3, 5))\n\nimportances_linear = []\nimportances_non_linear = []\nfor train, test in cv.split(X):\n    non_linear_model_ = clone(non_linear_model)\n    linear_model_ = clone(linear_model)\n    non_linear_model_.fit(X[train], y[train])\n    linear_model_.fit(X[train], y[train])\n\n    vim_linear = LOCO(\n        estimator=linear_model_, loss=log_loss, method=\"predict_proba\", n_jobs=2\n    )\n    vim_non_linear = LOCO(\n        estimator=non_linear_model_,\n        loss=hinge_loss,\n        method=\"decision_function\",\n        n_jobs=2,\n    )\n    vim_linear.fit(X[train], y[train])\n    vim_non_linear.fit(X[train], y[train])\n\n    importances_linear.append(vim_linear.importance(X[test], y[test])[\"importance\"])\n    importances_non_linear.append(\n        vim_non_linear.importance(X[test], y[test])[\"importance\"]\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To select variables using LOCO, we compute the p-values using a t-test over the\nimportance scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, pval_linear = ttest_1samp(importances_linear, 0, axis=0, alternative=\"greater\")\n_, pval_non_linear = ttest_1samp(\n    importances_non_linear, 0, axis=0, alternative=\"greater\"\n)\n\ndf_pval = pd.DataFrame(\n    {\n        \"pval\": np.hstack([pval_dcrt, pval_linear, pval_non_linear]),\n        \"method\": [\"d0CRT\"] * 2 + [\"LOCO-linear\"] * 2 + [\"LOCO-non-linear\"] * 2,\n        \"Feature\": [\"X1\", \"X2\"] * 3,\n    }\n)\ndf_pval[\"minus_log10_pval\"] = -np.log10(df_pval[\"pval\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the $-log_{10}(pval)$ for each method and variable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nsns.barplot(\n    data=df_pval,\n    y=\"Feature\",\n    x=\"minus_log10_pval\",\n    hue=\"method\",\n    palette=\"muted\",\n    ax=ax,\n)\nax.set_xlabel(\"-$\\\\log_{10}(pval)$\")\nax.axvline(\n    -np.log10(0.05), color=\"k\", lw=3, linestyle=\"--\", label=\"-$\\\\log_{10}(0.05)$\"\n)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, when using linear models (d0CRT and LOCO-linear) that are misspecified,\nthe varibles are not selected. This highlights the benefit of using model-agnostic\nmethods such as LOCO, which allows for the use of models that are expressive enough\nto explain the data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}