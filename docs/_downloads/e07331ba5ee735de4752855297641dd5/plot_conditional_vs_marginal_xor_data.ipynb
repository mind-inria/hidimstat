{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conditional vs Marginal Importance on the XOR dataset\n\nThis example illustrates on XOR data that variables can be conditionally important even\nif they are not marginally important. The conditional importance is computed using the\nConditional Permutation Importance (CPI) class and the marginal importance is computed using univariate models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.base import clone\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import hinge_loss\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.svm import SVC\n\nfrom hidimstat import CPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To solve the XOR problem, we will use a Support Vector Classier (SVC) with Radial Basis Function (RBF) kernel. The decision function of\nthe fitted model shows that the model is able to separate the two classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(0)\nX = rng.randn(400, 2)\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0).astype(int)\n\nxx, yy = np.meshgrid(\n    np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100),\n    np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100),\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\nmodel = SVC(kernel=\"rbf\", random_state=0)\nmodel.fit(X_train, y_train)\nZ = model.decision_function(np.c_[xx.ravel(), yy.ravel()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the decision function of the SVC\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nax.imshow(\n    Z.reshape(xx.shape),\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    cmap=\"RdYlBu_r\",\n    alpha=0.6,\n    origin=\"lower\",\n    aspect=\"auto\",\n)\nsns.scatterplot(\n    x=X[:, 0],\n    y=X[:, 1],\n    hue=Y,\n    ax=ax,\n    palette=\"muted\",\n)\nax.axis(\"off\")\nax.legend(\n    title=\"Class\",\n)\nax.set_title(\"Decision function of SVC with RBF kernel\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decision function of the SVC shows that the model is able to learn the\nnon-linear decision boundary of the XOR problem. It also highlights that knowing\nthe value of both features is necessary to classify each sample correctly.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the conditional and marginal importance\nWe first compute the marginal importance by fitting univariate models on each feature.\nThen, we compute the conditional importance using the CPI class. The univarariate\nmodels don't perform above chance, since solving the XOR problem requires to use both\nfeatures. Conditional importance, on the other hand, reveals that both features\nare important (therefore rejecting the null hypothesis\n$Y \\perp\\!\\!\\!\\perp X^1 | X^2$).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=0)\nclf = SVC(kernel=\"rbf\", random_state=0)\n# Compute marginal importance using univariate models\nmarginal_scores = []\nfor i in range(X.shape[1]):\n    feat_scores = []\n    for train_index, test_index in cv.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = Y[train_index], Y[test_index]\n\n        X_train_univariate = X_train[:, i].reshape(-1, 1)\n        X_test_univariate = X_test[:, i].reshape(-1, 1)\n\n        univariate_model = clone(clf)\n        univariate_model.fit(X_train_univariate, y_train)\n\n        feat_scores.append(univariate_model.score(X_test_univariate, y_test))\n    marginal_scores.append(feat_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "importances = []\nfor i, (train_index, test_index) in enumerate(cv.split(X)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = Y[train_index], Y[test_index]\n\n    clf_c = clone(clf)\n    clf_c.fit(X_train, y_train)\n\n    vim = CPI(\n        estimator=clf_c,\n        method=\"decision_function\",\n        loss=hinge_loss,\n        imputation_model_continuous=RidgeCV(np.logspace(-3, 3, 10)),\n        n_permutations=50,\n        random_state=0,\n    )\n    vim.fit(X_train, y_train)\n    importances.append(vim.importance(X_test, y_test)[\"importance\"])\n\nimportances = np.array(importances).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the importance scores\nWe will use boxplots to visualize the distribution of the importance scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(6, 2.5))\n# Marginal scores boxplot\nsns.boxplot(\n    data=np.array(marginal_scores).T,\n    orient=\"h\",\n    ax=axes[0],\n    fill=False,\n    color=\"C0\",\n    linewidth=3,\n)\naxes[0].axvline(x=0.5, color=\"k\", linestyle=\"--\", lw=3)\naxes[0].set_ylabel(\"\")\naxes[0].set_yticks([0, 1], [\"X1\", \"X2\"])\naxes[0].set_xlabel(\"Marginal Scores (accuracy)\")\naxes[0].set_ylabel(\"Features\")\n\n# Importances boxplot\nsns.boxplot(\n    data=np.array(importances).T,\n    orient=\"h\",\n    ax=axes[1],\n    fill=False,\n    color=\"C0\",\n    linewidth=3,\n)\naxes[1].set_xlabel(\"Conditional Importance\")\naxes[1].axvline(x=0.0, color=\"k\", linestyle=\"--\", lw=3)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the left, we can see that both features are not marginally important, since the\nboxplots overlap with the chance level (accuracy = 0.5). On the right, we can see that\nboth features are conditionally important, since the importance scores are far from\nthe null hypothesis (importance = 0.0).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}