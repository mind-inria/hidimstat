{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Measuring variable importance in classification\n\nIn this example, we illustrate how to measure variable importance in a classification \ncontext. The problem under consideration is a binary classification where the target \nvariable is generated using a non-linear function of the features. Therefore \nillustrating the importance of model-agnostic variable importance methods, which, as \nopposed to linear models for instance, can capture non-linear relationships. \nThe features are generated from a multivariate normal distribution with a Toeplitz \ncorrelation matrix. This second specificity of the problem is interesting to exemplify \nthe benefits of the conditional permutation importance (CPI) method [:footcite:t:`Chamma_NeurIPS2023`] \nover the standard permutation importance (PI) method [:footcite:t:`breimanRandomForests2001`].\n\n## References\n.. footbibliography::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports needed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.lines as mlines\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.linalg import toeplitz\nfrom scipy.stats import ttest_1samp\nfrom sklearn.base import clone\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import hinge_loss\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\nfrom sklearn.svm import SVC\n\nfrom hidimstat import CPI, PermutationImportance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the data\nWe generate the data using a multivariate normal distribution with a Toeplitz\ncorrelation matrix. The target variable is generated using a non-linear function\nof the features. To make the problem more intuitive, we generate a non-linear\ncombination of the features inspired by the Body Mass Index (BMI) formula.\nThe BMI can be obtained by $\\text{BMI} = \\frac{\\text{weight}}{\\text{height}^2}$.\nAnd we simply mimic the weight and height variables by rescalling 2 correlated\nfeatures. The binary target is then generated using the formula:\n$y = \\beta_1 \\exp\\left(\\frac{|\\text{bmi} - \\text{mean(bmi)}|}{\\text{std(bmi)}}\\right) + \\beta_2 \\exp\\left(|\\text{weight}| \\times 1\\left[|\\text{weight} - \\text{mean(weight)}| > \\text{quantile(weight, 0.80)}\\right] \\right) + \\beta_3 \\cdot \\text{age} + \\epsilon$ where $\\epsilon$` is a Gaussian noise.\nThe first and second term are non-linear functions of the features, corresponding to\ndeviations from the population mean while the third term is a linear function of a\nfeature.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 0\nrng = np.random.RandomState(seed)\nn_samples = 400\nn_features = 10\ncorr_mat = toeplitz(\n    np.linspace(1, 0.0, n_features) ** 2,\n)\nmean = np.zeros(corr_mat.shape[0])\nX = rng.multivariate_normal(\n    mean,\n    corr_mat,\n    size=n_samples,\n)\nnoise_y = 0.5\nweight = X[:, 4] * 20 + 74\nweight[weight < 40] += 30\nheight = X[:, 2] * 0.12 + 1.7\nage = X[:, 6] * 10 + 50\nbmi = weight / height**2\n\na = np.exp(np.abs(bmi - np.mean(bmi)) / np.std(bmi))\nb = np.exp(\n    np.abs(X[:, 4])\n    * ((X[:, 4] < np.quantile(X[:, 4], 0.10)) + (X[:, 4] > np.quantile(X[:, 4], 0.90)))\n)\n\ny_cont = 4 * a + 2.0 * b + 0.5 * age + noise_y * rng.randn(n_samples)\ny = y_cont > np.quantile(y_cont, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(\n    1,\n    2,\n    figsize=(8, 4),\n)\naxes[0].scatter(weight, height, c=y, cmap=\"coolwarm\")\naxes[0].set_xlabel(\"Weight\")\naxes[0].set_ylabel(\"Height\")\naxes[1].matshow(\n    corr_mat,\n)\nlabels = np.array(\n    [\n        \"X1\",\n        \"X2\",\n        \"Height\",\n        \"X4\",\n        \"Weight\",\n        \"X6\",\n        \"Age\",\n        \"X8\",\n        \"X9\",\n        \"X10\",\n    ]\n)\ntck_ids = np.arange(0, n_features)\naxes[1].set_xticks(tck_ids, labels[tck_ids], rotation=45)\naxes[1].set_yticks(tck_ids, labels[tck_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable importance inference\nWe use two different Support Vector Machine models, one with a linear kernel and\none with a polynomial kernel of degree 2, well specified to capture the non-linear\nrelationship between the features and the target variable. We then use the CPI and\nPI methods to compute the variable importance. We use a 5-fold cross-validation to\nestimate the importance of the features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 0\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nimportance_linear = []\nimportance_non_linear = []\nimportant_pi = []\n\nmodel_linear = RandomizedSearchCV(\n    SVC(random_state=seed, kernel=\"linear\"),\n    param_distributions={\n        \"C\": np.logspace(-3, 3, 10),\n    },\n    n_iter=10,\n    n_jobs=5,\n    random_state=seed,\n)\nmodel_non_linear = RandomizedSearchCV(\n    SVC(\n        random_state=seed,\n        kernel=\"poly\",\n        degree=2,\n        coef0=1,\n    ),\n    param_distributions={\n        \"C\": np.logspace(-3, 3, 10),\n    },\n    n_iter=10,\n    n_jobs=5,\n    random_state=seed,\n)\nimputation_model = RidgeCV(alphas=np.logspace(-3, 3, 50))\n\n\nimportance_list = []\nfor train, test in cv.split(X, y):\n    model_linear_c = clone(model_linear)\n    model_linear_c.fit(X[train], y[train])\n    cpi_linear = CPI(\n        estimator=model_linear_c,\n        imputation_model=clone(imputation_model),\n        n_permutations=50,\n        n_jobs=5,\n        loss=hinge_loss,\n        random_state=seed,\n        method=\"decision_function\",\n    )\n    cpi_linear.fit(X[train], y[train])\n    imp_cpi_linear = cpi_linear.score(X[test], y[test])[\"importance\"]\n\n    model_non_linear_c = clone(model_non_linear)\n    model_non_linear_c.fit(X[train], y[train])\n    cpi_non_linear = CPI(\n        estimator=model_non_linear_c,\n        imputation_model=clone(imputation_model),\n        n_permutations=50,\n        n_jobs=5,\n        loss=hinge_loss,\n        random_state=seed,\n        method=\"decision_function\",\n    )\n    cpi_non_linear.fit(X[train], y[train])\n    imp_cpi_non_linear = cpi_non_linear.score(X[test], y[test])[\"importance\"]\n\n    pi_non_linear = PermutationImportance(\n        estimator=model_non_linear_c,\n        n_permutations=50,\n        n_jobs=5,\n        random_state=seed,\n        method=\"decision_function\",\n    )\n    pi_non_linear.fit(X[train], y[train])\n    imp_pi_non_linear = pi_non_linear.score(X[test], y[test])[\"importance\"]\n\n    importance_list.append(\n        np.stack(\n            [\n                imp_cpi_linear,\n                imp_cpi_non_linear,\n                imp_pi_non_linear,\n            ]\n        )\n    )\n\nimportance_arr = np.stack(importance_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the p-values for the variable importance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pval_arr = np.zeros((n_features, 3))\nfor j in range(n_features):\n    for i in range(3):\n        diff = importance_arr[:, i, j]\n        pval_arr[j, i] = ttest_1samp(diff, 0)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the variable importance\nHere we plot the variable importance and highlight the features that are considered\nimportant, with a p-value lower than 0.05, using a diamond marker. We also highlight\nthe true important features, used to generate the target variable, with a star marker.\nWhile the linear model captures the importance of the age, it fails to capture the\nimportance of the weight and height because of its lack of expressivity. Using a\npolynomial kernel, the non-linear model captures the importance of the weight and height.\nFinally, the CPI method controls for false positive discoveries contrarily to the PI method\nwhich identifies spurious important features simply because of the correlation structure of\nthe features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nbox1 = ax.boxplot(\n    importance_arr[:, 0, :],\n    positions=np.arange(1, n_features + 1) - 0.25,\n    widths=0.2,\n    label=\"CPI Linear\",\n    vert=False,\n)\nfor item in [\"whiskers\", \"fliers\", \"medians\", \"caps\", \"boxes\"]:\n    plt.setp(box1[item], color=\"tab:orange\")\n\nbox1 = ax.boxplot(\n    importance_arr[:, 1, :],\n    positions=np.arange(1, n_features + 1),\n    widths=0.2,\n    label=\"CPI Poly\",\n    vert=False,\n)\nfor item in [\"whiskers\", \"fliers\", \"medians\", \"caps\", \"boxes\"]:\n    plt.setp(box1[item], color=\"tab:blue\")\n\nbox1 = ax.boxplot(\n    importance_arr[:, 2, :],\n    positions=np.arange(1, n_features + 1) + 0.25,\n    widths=0.2,\n    label=\"PI Poly\",\n    vert=False,\n)\nfor item in [\"whiskers\", \"fliers\", \"medians\", \"caps\", \"boxes\"]:\n    plt.setp(box1[item], color=\"tab:green\")\n\nax.set_yticks(np.arange(1, n_features + 1), labels)\nax.legend()\nax.axvline(0, color=\"black\", lw=1, ls=\"--\", zorder=-1)\nax.set_xlabel(\"Importance\")\n\n# Plot the important features based on thresholded p-values\nthreshold = 0.05\nfor j in range(n_features):\n    for i, color in enumerate([\"tab:orange\", \"tab:blue\", \"tab:green\"]):\n\n        if pval_arr[j, i] < threshold:\n            ax.plot(\n                2 + (i - 1) * 0.5,\n                j + 1,\n                marker=\"D\",\n                color=color,\n                markersize=8,\n                zorder=3,\n            )\n    if j in [2, 4, 6]:\n        ax.plot(\n            3,\n            j + 1,\n            marker=\"*\",\n            color=\"k\",\n            markersize=10,\n            zorder=3,\n        )\nimportant_legend = mlines.Line2D(\n    [],\n    [],\n    color=\"grey\",\n    marker=\"D\",\n    linestyle=\"None\",\n    markersize=8,\n    label=f\"p-value < {threshold}\",\n)\nground_truth_legend = mlines.Line2D(\n    [], [], color=\"k\", marker=\"*\", linestyle=\"None\", markersize=10, label=\"Ground Truth\"\n)\nhandles = ax.get_legend_handles_labels()[0]\nax.legend(handles=handles + [important_legend, ground_truth_legend], loc=\"upper right\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}